{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e690aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid symbols: ['UBSG.SW', 'SREN.SW', 'LONN.SW', 'ALC.SW', 'GIVN.SW', 'CFR.SW', 'ABBN.SW', 'HOLN.SW', 'SCMN.SW', 'HELN.SW', 'SLHN.SW', 'SQN.SW', 'BKW.SW', 'REHN.SW', 'LOGN.SW', 'UHR.SW', 'LAND.SW', 'VONN.SW', 'BAER.SW', 'BLKB.SW', 'BALN.SW']\n",
      "Problem symbols: []\n",
      "status\n",
      "ok        21\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "TICKERS = {\n",
    "    \"large\": {\n",
    "        \"banking\": [\"UBSG.SW\"],\n",
    "        \"insurance\": [\"ZURN.SW\"],\n",
    "        \"pharma\": [\"ROG.SW\", \"NOVN.SW\", \"SANN.SW\"],\n",
    "        \"food_retail\": [\"NESN.SW\", \"EMMN.SW\", \"ARYN.SW\"],\n",
    "        \"insurance\": [\"SREN.SW\"],               # Swiss Re\n",
    "        \"pharma\": [\"LONN.SW\", \"ALC.SW\"],        # Lonza, Alcon\n",
    "        \"food_retail\": [\"GIVN.SW\"],             # Givaudan\n",
    "        \"consumer\": [\"CFR.SW\"],                 # Richemont\n",
    "        \"tech_industrial\": [\"ABBN.SW\", \"HOLN.SW\", \"SCMN.SW\"],  # ABB, Holcim, Swisscom\n",
    "    },\n",
    "    \"mid\": {\n",
    "        \"insurance\": [\"HELN.SW\", \"SLHN.SW\", \"SQN.SW\"],\n",
    "        \"energy\": [\"BKW.SW\", \"REHN.SW\"],\n",
    "        \"tech\": [\"LOGN.SW\", \"UHR.SW\", \"LAND.SW\"],\n",
    "    },\n",
    "    \"small\": {\n",
    "        \"banking\": [\"VONN.SW\", \"BAER.SW\", \"BLKB.SW\"],\n",
    "        \"insurance\": [\"BALN.SW\"],  # Mobiliar unlisted; add others as needed\n",
    "    },\n",
    "}\n",
    "\n",
    "def flatten(d):\n",
    "    rows = []\n",
    "    for size, sectors in d.items():\n",
    "        for sector, syms in sectors.items():\n",
    "            for s in syms:\n",
    "                rows.append({\"size\": size, \"sector\": sector, \"ticker\": s})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def validate_tickers(tickers):\n",
    "    ok, bad = [], []\n",
    "    for t in tickers:\n",
    "        try:\n",
    "            info = yf.Ticker(t).fast_info\n",
    "            # fast_info usually has fields when valid; empty dict for bad/OTC\n",
    "            if info and getattr(info, \"last_price\", None) is not None:\n",
    "                ok.append(t)\n",
    "            else:\n",
    "                bad.append(t)\n",
    "        except Exception:\n",
    "            bad.append(t)\n",
    "    return ok, bad\n",
    "\n",
    "def download_batch(tickers, start=\"2018-01-01\", end=\"2024-12-31\", out_dir=\"data/stock_prices\", max_retry=3):\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    manifest = []\n",
    "\n",
    "    # Batch download (fewer HTTP calls). Keep auto_adjust for total-return style close.\n",
    "    remaining = set(tickers)\n",
    "    attempt = 0\n",
    "    while remaining and attempt < max_retry:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            data = yf.download(\n",
    "                tickers=list(remaining),\n",
    "                start=start, end=end,\n",
    "                auto_adjust=True, group_by=\"ticker\", threads=True, progress=False\n",
    "            )\n",
    "        except Exception:\n",
    "            data = None\n",
    "\n",
    "        fetched = set()\n",
    "        if data is not None and not data.empty:\n",
    "            # When multiple: columns become (ticker, field)\n",
    "            for t in list(remaining):\n",
    "                try:\n",
    "                    df = data[t].dropna(how=\"all\")\n",
    "                    if not df.empty:\n",
    "                        df.to_csv(f\"{out_dir}/{t}.csv\")\n",
    "                        manifest.append({\"ticker\": t, \"status\": \"ok\", \"rows\": len(df)})\n",
    "                        fetched.add(t)\n",
    "                    else:\n",
    "                        manifest.append({\"ticker\": t, \"status\": \"empty\", \"rows\": 0})\n",
    "                        fetched.add(t)\n",
    "                except Exception:\n",
    "                    # Single-ticker case or structure mismatch: try direct download\n",
    "                    try:\n",
    "                        df2 = yf.download(t, start=start, end=end, auto_adjust=True, progress=False)\n",
    "                        if not df2.empty:\n",
    "                            df2.to_csv(f\"{out_dir}/{t}.csv\")\n",
    "                            manifest.append({\"ticker\": t, \"status\": \"ok_solo\", \"rows\": len(df2)})\n",
    "                            fetched.add(t)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "        remaining -= fetched\n",
    "        if remaining:\n",
    "            time.sleep(1.0 * attempt)  # backoff\n",
    "\n",
    "    # Anything still remaining → mark failed\n",
    "    for t in remaining:\n",
    "        manifest.append({\"ticker\": t, \"status\": \"failed\", \"rows\": 0})\n",
    "\n",
    "    pd.DataFrame(manifest).to_csv(f\"{out_dir}/_manifest.csv\", index=False)\n",
    "    return pd.DataFrame(manifest)\n",
    "\n",
    "# ---- Run it ----\n",
    "universe = flatten(TICKERS)\n",
    "ok_syms, bad_syms = validate_tickers(universe[\"ticker\"].tolist())\n",
    "print(\"Valid symbols:\", ok_syms)\n",
    "print(\"Problem symbols:\", bad_syms)\n",
    "\n",
    "manifest = download_batch(ok_syms, start=\"2018-01-01\", end=\"2024-12-31\", out_dir=\"data/stock_prices\")\n",
    "print(manifest.value_counts([\"status\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4eeb09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABBN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "ALC.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "ALPN.SW.csv → columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
      "ARYN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "BAER.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "BALN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "BKW.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "BLKB.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "CFR.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "EMMN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "GIVN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "HELN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "HOLN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "LAND.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "LOGN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "LONN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "NESN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "NOVN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "REHN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "ROG.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "SANN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "SCMN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "SLHN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "SQN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "SREN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "SWON.SW.csv → columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
      "UBSG.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "UHR.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "VONN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "ZURN.SW.csv → columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "for p in Path(\"data/stock_prices\").glob(\"*.csv\"):\n",
    "    if p.name.startswith(\"_\"): \n",
    "        continue\n",
    "    head = pd.read_csv(p, nrows=5)\n",
    "    print(p.name, \"→ columns:\", list(head.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78af3555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping (not in universe): ALPN.SW.csv\n",
      "Skipping (not in universe): SWON.SW.csv\n",
      "Normalized 28 files → data\\stock_prices_clean\n",
      "               min        max  count\n",
      "ticker                              \n",
      "SREN.SW 2018-01-03 2024-12-30   1759\n",
      "UBSG.SW 2018-01-03 2024-12-30   1759\n",
      "UHR.SW  2018-01-03 2024-12-30   1759\n",
      "VONN.SW 2018-01-03 2024-12-30   1759\n",
      "ZURN.SW 2018-01-03 2024-12-30   1759\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "VALID = {\n",
    "    \"UBSG.SW\",\"ZURN.SW\",\"ROG.SW\",\"NOVN.SW\",\"SANN.SW\",\n",
    "    \"NESN.SW\",\"EMMN.SW\",\"ARYN.SW\",\n",
    "    \"HELN.SW\",\"SLHN.SW\",\"SQN.SW\",\n",
    "    \"BKW.SW\",\"REHN.SW\",\n",
    "    \"LOGN.SW\",\"UHR.SW\",\"LAND.SW\",\n",
    "    \"VONN.SW\",\"BAER.SW\",\"BLKB.SW\",\"BALN.SW\",\n",
    "    \"SREN.SW\",\"CFR.SW\",\"LONN.SW\",\"HOLN.SW\",\n",
    "    \"SCMN.SW\",\"GIVN.SW\",\"ALC.SW\",\"ABBN.SW\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "RAW_DIR = Path(\"data/stock_prices\")\n",
    "CLEAN_DIR = Path(\"data/stock_prices_clean\")\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def normalize_csv(p: Path):\n",
    "    # skip non-universe files\n",
    "    ticker = p.stem\n",
    "    if ticker not in VALID:\n",
    "        print(f\"Skipping (not in universe): {p.name}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "    except Exception as e:\n",
    "        print(f\"Read fail {p.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"Empty file: {p.name}\")\n",
    "        return None\n",
    "\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    # Fix missing Date (some files have 'Price' as the first column)\n",
    "    if \"Date\" not in cols:\n",
    "        if \"Price\" in cols:\n",
    "            df = df.rename(columns={\"Price\": \"Date\"})\n",
    "        elif \"Unnamed: 0\" in cols:\n",
    "            df = df.rename(columns={\"Unnamed: 0\": \"Date\"})\n",
    "        else:\n",
    "            print(f\"No Date-like column in {p.name}, skipping.\")\n",
    "            return None\n",
    "\n",
    "    # Parse dates\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Date\"])\n",
    "\n",
    "    # Ensure required OHLCV columns exist\n",
    "    needed = {\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"}\n",
    "    missing = needed - set(df.columns)\n",
    "    if missing:\n",
    "        print(f\"Missing {missing} in {p.name}, skipping.\")\n",
    "        return None\n",
    "\n",
    "    # Build normalized DF\n",
    "    out = df[[\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]].copy()\n",
    "\n",
    "    # If Adj Close missing, use Close as a fallback (not total return)\n",
    "    if \"Adj Close\" in df.columns:\n",
    "        out[\"Adj Close\"] = df[\"Adj Close\"]\n",
    "    else:\n",
    "        out[\"Adj Close\"] = df[\"Close\"]\n",
    "\n",
    "    out = (out.rename(columns=str.lower)\n",
    "              .sort_values(\"date\")\n",
    "              .assign(ticker=ticker))\n",
    "\n",
    "    out.to_csv(CLEAN_DIR / p.name, index=False)\n",
    "    return out\n",
    "\n",
    "# Run normalization\n",
    "normalized = []\n",
    "for p in RAW_DIR.glob(\"*.csv\"):\n",
    "    if p.name.startswith(\"_\"):\n",
    "        continue\n",
    "    res = normalize_csv(p)\n",
    "    if res is not None:\n",
    "        normalized.append(res)\n",
    "\n",
    "if not normalized:\n",
    "    raise RuntimeError(\"No normalized files produced.\")\n",
    "\n",
    "print(f\"Normalized {len(normalized)} files → {CLEAN_DIR}\")\n",
    "\n",
    "# ---------- Rebuild the panel using the CLEAN_DIR ----------\n",
    "def load_prices_clean(data_dir=CLEAN_DIR):\n",
    "    data=[]\n",
    "    for p in Path(data_dir).glob(\"*.csv\"):\n",
    "        df = pd.read_csv(p, parse_dates=[\"date\"])\n",
    "        if df.empty: \n",
    "            continue\n",
    "        data.append(df[[\"date\",\"open\",\"high\",\"low\",\"close\",\"adj close\",\"volume\",\"ticker\"]]\n",
    "                      .rename(columns={\"adj close\":\"adj_close\"}))\n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "px = load_prices_clean()\n",
    "print(px.groupby(\"ticker\")[\"date\"].agg(['min','max','count']).sort_index().tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8796f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date       open       high        low      close  adj_close  \\\n",
      "0 2018-01-03  19.562587  19.749111  19.547665  19.719267  19.719267   \n",
      "1 2018-01-04  19.838642  19.958017  19.749111  19.935635  19.935635   \n",
      "2 2018-01-05  19.920712  20.084852  19.883406  20.084852  20.084852   \n",
      "3 2018-01-08  20.099775  20.137079  19.920713  20.092314  20.092314   \n",
      "4 2018-01-09  19.972938  20.196766  19.935634  20.196766  20.196766   \n",
      "\n",
      "      volume   ticker  \n",
      "0  5519259.0  ABBN.SW  \n",
      "1  5738092.0  ABBN.SW  \n",
      "2  4435594.0  ABBN.SW  \n",
      "3  5029780.0  ABBN.SW  \n",
      "4  6974533.0  ABBN.SW  \n",
      "28 tickers loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def load_prices_clean(data_dir=\"data/stock_prices_clean\"):\n",
    "    data=[]\n",
    "    for p in Path(data_dir).glob(\"*.csv\"):\n",
    "        df = pd.read_csv(p, parse_dates=[\"date\"])\n",
    "        if df.empty: \n",
    "            continue\n",
    "        df = df.rename(columns={\"adj close\":\"adj_close\"})\n",
    "        data.append(df[[\"date\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\",\"ticker\"]])\n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "panel = load_prices_clean()\n",
    "print(panel.head())\n",
    "print(panel[\"ticker\"].nunique(), \"tickers loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f81c7c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Reloaded panel with features: (48908, 10)\n",
      "\n",
      "Missing ratio per ticker (0 = perfect):\n",
      " ticker\n",
      "ABBN.SW    0.0\n",
      "ALC.SW     0.0\n",
      "ARYN.SW    0.0\n",
      "BAER.SW    0.0\n",
      "BALN.SW    0.0\n",
      "BKW.SW     0.0\n",
      "BLKB.SW    0.0\n",
      "CFR.SW     0.0\n",
      "EMMN.SW    0.0\n",
      "GIVN.SW    0.0\n",
      "HELN.SW    0.0\n",
      "HOLN.SW    0.0\n",
      "LAND.SW    0.0\n",
      "LOGN.SW    0.0\n",
      "LONN.SW    0.0\n",
      "NESN.SW    0.0\n",
      "NOVN.SW    0.0\n",
      "REHN.SW    0.0\n",
      "ROG.SW     0.0\n",
      "SANN.SW    0.0\n",
      "SCMN.SW    0.0\n",
      "SLHN.SW    0.0\n",
      "SQN.SW     0.0\n",
      "SREN.SW    0.0\n",
      "UBSG.SW    0.0\n",
      "UHR.SW     0.0\n",
      "VONN.SW    0.0\n",
      "ZURN.SW    0.0\n",
      "Name: adj_close, dtype: float64\n",
      "\n",
      "Any non-monotonic sequences? False\n",
      "\n",
      "Gaps vs UBSG.SW calendar (top 10):\n",
      " ALC.SW     316\n",
      "ABBN.SW      0\n",
      "ARYN.SW      0\n",
      "BAER.SW      0\n",
      "BALN.SW      0\n",
      "BKW.SW       0\n",
      "BLKB.SW      0\n",
      "CFR.SW       0\n",
      "EMMN.SW      0\n",
      "GIVN.SW      0\n",
      "dtype: int64\n",
      "\n",
      "Extreme return quantiles:\n",
      "             0.001     0.010     0.990     0.999\n",
      "ticker                                         \n",
      "ABBN.SW -0.076706 -0.043446  0.040719  0.061835\n",
      "ALC.SW  -0.076492 -0.042929  0.051251  0.129585\n",
      "ARYN.SW -0.214818 -0.096395  0.103197  0.172997\n",
      "BAER.SW -0.098513 -0.055243  0.050565  0.091927\n",
      "BALN.SW -0.078293 -0.040491  0.032343  0.066443\n",
      "BKW.SW  -0.070783 -0.035999  0.041748  0.086066\n",
      "BLKB.SW -0.018212 -0.011347  0.011382  0.018246\n",
      "CFR.SW  -0.110768 -0.052241  0.054122  0.110484\n",
      "EMMN.SW -0.069052 -0.035745  0.039179  0.088726\n",
      "GIVN.SW -0.065999 -0.036661  0.031367  0.069301\n",
      "HELN.SW -0.123187 -0.043824  0.039526  0.086410\n",
      "HOLN.SW -0.086583 -0.041648  0.038913  0.055429\n",
      "LAND.SW -0.105842 -0.055148  0.047560  0.095826\n",
      "LOGN.SW -0.126543 -0.061190  0.051043  0.126515\n",
      "LONN.SW -0.119991 -0.049233  0.049509  0.080977\n",
      "NESN.SW -0.053232 -0.028302  0.024230  0.042916\n",
      "NOVN.SW -0.050468 -0.031179  0.030188  0.054376\n",
      "REHN.SW -0.066943 -0.040588  0.045326  0.072140\n",
      "ROG.SW  -0.058393 -0.033390  0.031367  0.054781\n",
      "SANN.SW -0.276168 -0.106911  0.162341  0.492840\n",
      "SCMN.SW -0.066509 -0.028320  0.025206  0.053297\n",
      "SLHN.SW -0.090632 -0.042498  0.037203  0.066047\n",
      "SQN.SW  -0.127988 -0.066042  0.069226  0.140278\n",
      "SREN.SW -0.088954 -0.043395  0.037256  0.075557\n",
      "UBSG.SW -0.097148 -0.050125  0.053411  0.090324\n",
      "UHR.SW  -0.083768 -0.051618  0.050594  0.086887\n",
      "VONN.SW -0.087509 -0.050678  0.043522  0.087969\n",
      "ZURN.SW -0.076119 -0.038799  0.031441  0.063988\n",
      "\n",
      "Top absolute daily moves:\n",
      "       date  ticker    ret_1d\n",
      "2019-02-25 SANN.SW  1.241681\n",
      "2021-06-01 SANN.SW  0.563726\n",
      "2022-12-20 SANN.SW  0.470086\n",
      "2022-12-19 SANN.SW  0.409638\n",
      "2019-02-26 SANN.SW  0.400000\n",
      "2018-01-24 SANN.SW -0.317955\n",
      "2022-12-21 SANN.SW -0.308140\n",
      "2018-05-24 ARYN.SW -0.267212\n",
      "2020-10-06 SANN.SW -0.265905\n",
      "2020-03-12 ARYN.SW -0.233746\n",
      "2022-12-16 SANN.SW  0.231454\n",
      "2018-03-05 SANN.SW -0.226306\n",
      "2023-05-23 SANN.SW  0.221374\n",
      "2023-06-02 SANN.SW  0.210823\n",
      "2022-12-15 SANN.SW  0.210054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkcak\\AppData\\Local\\Temp\\ipykernel_30120\\2569322819.py:25: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  panel = panel.groupby(\"ticker\", group_keys=False).apply(add_features).dropna().reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- 1) Reload cleaned prices ----\n",
    "def load_prices_clean(data_dir=\"data/stock_prices_clean\"):\n",
    "    data=[]\n",
    "    for p in Path(data_dir).glob(\"*.csv\"):\n",
    "        df = pd.read_csv(p, parse_dates=[\"date\"])\n",
    "        if df.empty: \n",
    "            continue\n",
    "        df = df.rename(columns={\"adj close\":\"adj_close\"})\n",
    "        data.append(df[[\"date\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\",\"ticker\"]])\n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "panel = load_prices_clean()\n",
    "\n",
    "# ---- 2) Add basic features (so QC has ret_1d, etc.) ----\n",
    "def add_features(g):\n",
    "    g = g.sort_values(\"date\").copy()\n",
    "    g[\"ret_1d\"] = g[\"adj_close\"].pct_change()\n",
    "    g[\"logret_1d\"] = np.log(g[\"adj_close\"]).diff()\n",
    "    return g\n",
    "\n",
    "panel = panel.groupby(\"ticker\", group_keys=False).apply(add_features).dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"✅ Reloaded panel with features:\", panel.shape)\n",
    "\n",
    "# ---- 3) QC checks ----\n",
    "# a) Missing data ratios\n",
    "miss = (panel\n",
    "        .groupby(\"ticker\")[\"adj_close\"]\n",
    "        .apply(lambda s: s.isna().mean())\n",
    "        .sort_values(ascending=False))\n",
    "print(\"\\nMissing ratio per ticker (0 = perfect):\\n\", miss)\n",
    "\n",
    "# b) Trading-day alignment\n",
    "non_mono = panel.groupby(\"ticker\")[\"date\"].apply(lambda s: not s.is_monotonic_increasing)\n",
    "print(\"\\nAny non-monotonic sequences?\", bool(non_mono.any()))\n",
    "\n",
    "ref_dates = set(panel.loc[panel.ticker.eq(\"UBSG.SW\"), \"date\"])\n",
    "gap_counts = {t: len(ref_dates - set(g[\"date\"])) for t, g in panel.groupby(\"ticker\")}\n",
    "print(\"\\nGaps vs UBSG.SW calendar (top 10):\\n\", pd.Series(gap_counts).sort_values(ascending=False).head(10))\n",
    "\n",
    "# c) Return outliers\n",
    "q = panel.groupby(\"ticker\")[\"ret_1d\"].quantile([0.001,0.01,0.99,0.999]).unstack()\n",
    "print(\"\\nExtreme return quantiles:\\n\", q)\n",
    "\n",
    "abs_out = (panel.assign(abs_ret=lambda d: d[\"ret_1d\"].abs())\n",
    "           .sort_values(\"abs_ret\", ascending=False)\n",
    "           .loc[:,[\"date\",\"ticker\",\"ret_1d\"]]\n",
    "           .head(15))\n",
    "print(\"\\nTop absolute daily moves:\\n\", abs_out.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6cb7ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calendar gaps (should be 0 after alignment):\n",
      " ticker\n",
      "ALC.SW     316\n",
      "ABBN.SW      0\n",
      "ARYN.SW      0\n",
      "BAER.SW      0\n",
      "BALN.SW      0\n",
      "Name: date, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 0) START from your current `panel` (already loaded and has ret_1d/logret_1d) ---\n",
    "\n",
    "# A) (Optional) Align to a common trading calendar (drop days missing for reference ticker)\n",
    "#    This avoids forward-filling prices across non-trading days for thin names.\n",
    "REF = \"UBSG.SW\"\n",
    "ref_dates = set(panel.loc[panel.ticker.eq(REF), \"date\"])\n",
    "panel_aligned = panel.loc[panel[\"date\"].isin(ref_dates)].copy()\n",
    "\n",
    "# Quick check\n",
    "cal_gaps = (panel_aligned.groupby(\"ticker\")[\"date\"]\n",
    "            .apply(lambda s: len(ref_dates - set(s))))\n",
    "print(\"Calendar gaps (should be 0 after alignment):\\n\", cal_gaps.sort_values(ascending=False).head())\n",
    "\n",
    "# B) Winsorize daily returns per ticker (robust to biotech-style spikes)\n",
    "#    We'll cap at the 0.5% / 99.5% quantiles PER TICKER.\n",
    "def winsorize_group(g, col=\"ret_1d\", lower_q=0.005, upper_q=0.995):\n",
    "    g = g.sort_values(\"date\").copy()\n",
    "    lo, hi = g[col].quantile([lower_q, upper_q]).values\n",
    "    g[col + \"_w\"] = g[col].clip(lo, hi)\n",
    "    # Recompute log returns consistently from winsorized simple returns\n",
    "    g[\"logret_1d_w\"] = np.log1p(g[col + \"_w\"])\n",
    "    return g\n",
    "\n",
    "# Use the more explicit include_groups=False to silence future warning\n",
    "panel_w = (panel_aligned\n",
    "           .groupby(\"ticker\", group_keys=False, sort=False)\n",
    "           .apply(winsorize_group, include_groups=False))\n",
    "\n",
    "# C) Recompute rolling features based on winsorized log returns (leakage-safe)\n",
    "def add_roll_feats(g):\n",
    "    g = g.sort_values(\"date\").copy()\n",
    "    # Annualized realized vol on winsorized log returns\n",
    "    for w in (5, 21, 63):\n",
    "        g[f\"rv_{w}d_w\"] = g[\"logret_1d_w\"].rolling(w).std() * np.sqrt(252)\n",
    "        g[f\"ma_{w}d\"] = g[\"adj_close\"].rolling(w).mean()\n",
    "        g[f\"mom_{w}d\"] = g[\"adj_close\"].pct_change(w)\n",
    "    # Keep existing targets if you have them; if not, you can (re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77591eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_w = (panel_aligned\n",
    "           .groupby(\"ticker\", group_keys=False, sort=False)\n",
    "           .apply(winsorize_group, include_groups=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bd11d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'ret_1d',\n",
      "       'logret_1d', 'ret_1d_w', 'logret_1d_w'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(panel_w.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e19375f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkcak\\AppData\\Local\\Temp\\ipykernel_30120\\211234407.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(winsorize_group))\n",
      "C:\\Users\\mkcak\\AppData\\Local\\Temp\\ipykernel_30120\\211234407.py:55: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(add_roll_feats))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ panel_w rebuilt. Columns: ['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'ticker', 'ret_1d', 'logret_1d'] ... (total: 23 )\n",
      "Has ticker? True\n",
      "✅ Saved to: data\\stock_prices_ready\n",
      "Wide shape: (1675, 28)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- assume you already have `panel` with columns incl. 'ticker' ---\n",
    "# If not, reload from clean CSVs:\n",
    "# from pathlib import Path\n",
    "# def load_prices_clean(data_dir=\"data/stock_prices_clean\"):\n",
    "#     data=[]\n",
    "#     for p in Path(data_dir).glob(\"*.csv\"):\n",
    "#         df = pd.read_csv(p, parse_dates=[\"date\"]).rename(columns={\"adj close\":\"adj_close\"})\n",
    "#         data.append(df[[\"date\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\",\"ticker\"]])\n",
    "#     return pd.concat(data, ignore_index=True)\n",
    "# panel = load_prices_clean()\n",
    "\n",
    "# 1) Align to UBSG calendar (as before)\n",
    "REF = \"UBSG.SW\"\n",
    "ref_dates = set(panel.loc[panel.ticker.eq(REF), \"date\"])\n",
    "panel_aligned = panel.loc[panel[\"date\"].isin(ref_dates)].copy()\n",
    "\n",
    "# 2) Winsorize per-ticker and KEEP ticker via group name\n",
    "def winsorize_group(g, col=\"ret_1d\", lower_q=0.005, upper_q=0.995):\n",
    "    t = g.name  # <-- group/ticker name\n",
    "    g = g.sort_values(\"date\").copy()\n",
    "    # ensure ticker present\n",
    "    g[\"ticker\"] = t\n",
    "    # if ret_1d/logret_1d missing, create them\n",
    "    if \"ret_1d\" not in g:\n",
    "        g[\"ret_1d\"] = g[\"adj_close\"].pct_change()\n",
    "    if \"logret_1d\" not in g:\n",
    "        g[\"logret_1d\"] = np.log(g[\"adj_close\"]).diff()\n",
    "    lo, hi = g[\"ret_1d\"].quantile([lower_q, upper_q]).values\n",
    "    g[\"ret_1d_w\"] = g[\"ret_1d\"].clip(lo, hi)\n",
    "    g[\"logret_1d_w\"] = np.log1p(g[\"ret_1d_w\"])\n",
    "    return g\n",
    "\n",
    "panel_w = (panel_aligned\n",
    "           .groupby(\"ticker\", group_keys=False, sort=False)\n",
    "           .apply(winsorize_group))\n",
    "\n",
    "# 3) Rolling features + targets (again ensuring ticker stays)\n",
    "def add_roll_feats(g):\n",
    "    t = g.name\n",
    "    g = g.sort_values(\"date\").copy()\n",
    "    g[\"ticker\"] = t\n",
    "    for w in (5, 21, 63):\n",
    "        g[f\"rv_{w}d_w\"] = g[\"logret_1d_w\"].rolling(w).std() * np.sqrt(252)\n",
    "        g[f\"ma_{w}d\"]   = g[\"adj_close\"].rolling(w).mean()\n",
    "        g[f\"mom_{w}d\"]  = g[\"adj_close\"].pct_change(w)\n",
    "    for h in (5, 21):\n",
    "        g[f\"y_fwdret_{h}d\"] = g[\"adj_close\"].pct_change(h).shift(-h)\n",
    "    return g\n",
    "\n",
    "panel_w = (panel_w\n",
    "           .groupby(\"ticker\", group_keys=False, sort=False)\n",
    "           .apply(add_roll_feats))\n",
    "\n",
    "need_cols = [\"ticker\",\"date\",\"ret_1d_w\",\"logret_1d_w\",\"rv_5d_w\",\"rv_21d_w\",\"rv_63d_w\",\"y_fwdret_5d\",\"y_fwdret_21d\"]\n",
    "panel_w = panel_w.dropna(subset=need_cols).reset_index(drop=True)\n",
    "\n",
    "print(\"✅ panel_w rebuilt. Columns:\", list(panel_w.columns)[:10], \"... (total:\", len(panel_w.columns), \")\")\n",
    "print(\"Has ticker?\", \"ticker\" in panel_w.columns)\n",
    "\n",
    "# 4) Save tidy + wide\n",
    "from pathlib import Path\n",
    "out_dir = Path(\"data/stock_prices_ready\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "panel_w.to_parquet(out_dir / \"swiss_panel_ready.parquet\", index=False)\n",
    "panel_w.to_csv(out_dir / \"swiss_panel_ready.csv\", index=False)\n",
    "\n",
    "close_wide = (panel_w\n",
    "              .pivot(index=\"date\", columns=\"ticker\", values=\"adj_close\")\n",
    "              .sort_index())\n",
    "close_wide.to_parquet(out_dir / \"swiss_close_wide_ready.parquet\")\n",
    "close_wide.to_csv(out_dir / \"swiss_close_wide_ready.csv\")\n",
    "\n",
    "print(\"✅ Saved to:\", out_dir)\n",
    "print(\"Wide shape:\", close_wide.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f4098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 28 tickers; (48936, 8) rows\n",
      "After calendar alignment: (48936, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkcak\\AppData\\Local\\Temp\\ipykernel_30120\\2058132570.py:86: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  panel_w = panel.groupby(\"ticker\", group_keys=False).apply(add_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched panel: (46584, 23)\n",
      "✅ Saved to: C:\\Users\\mkcak\\OneDrive - Hochschule Luzern\\Modules\\master_thesis\\multimodal-finance-forecasting\\noteebooks\\data\\stock_prices_ready\n",
      "Wide shape: (1675, 28)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# === 0) Configure paths ===\n",
    "DATA_DIR = Path(r\"C:\\Users\\mkcak\\OneDrive - Hochschule Luzern\\Modules\\master_thesis\\multimodal-finance-forecasting\\noteebooks\\data\\stock_prices_clean\")\n",
    "OUT_DIR  = DATA_DIR.parent / \"stock_prices_ready\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === 1) Load all cleaned CSVs robustly (keeps ticker) ===\n",
    "def load_prices_clean(data_dir: Path):\n",
    "    dfs = []\n",
    "    for p in data_dir.glob(\"*.csv\"):\n",
    "        df = pd.read_csv(p)\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # Normalize date column name + parse\n",
    "        if \"date\" in df.columns:\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        elif \"Date\" in df.columns:\n",
    "            df[\"date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "        else:\n",
    "            print(f\"⚠️ No date column in {p.name}, skipping\")\n",
    "            continue\n",
    "        df = df.dropna(subset=[\"date\"])\n",
    "\n",
    "        # Normalize price columns (handle 'Adj Close' vs 'adj close', etc.)\n",
    "        cols_lower = {c: c.lower() for c in df.columns}\n",
    "        df = df.rename(columns=cols_lower)\n",
    "        if \"adj close\" in df.columns and \"adj_close\" not in df.columns:\n",
    "            df = df.rename(columns={\"adj close\": \"adj_close\"})\n",
    "        if \"adj_close\" not in df.columns:\n",
    "            # Fallback to close if adj not present\n",
    "            if \"close\" in df.columns:\n",
    "                df[\"adj_close\"] = df[\"close\"]\n",
    "            else:\n",
    "                print(f\"⚠️ No adj_close/close in {p.name}, skipping\")\n",
    "                continue\n",
    "\n",
    "        # Ensure ticker column\n",
    "        if \"ticker\" not in df.columns:\n",
    "            df[\"ticker\"] = p.stem  # use filename\n",
    "\n",
    "        keep = [\"date\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\",\"ticker\"]\n",
    "        df = df[[c for c in keep if c in df.columns]]\n",
    "        dfs.append(df)\n",
    "    if not dfs:\n",
    "        raise RuntimeError(f\"No usable CSVs found in {data_dir}\")\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "panel = load_prices_clean(DATA_DIR)\n",
    "panel = panel.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n",
    "print(\"Loaded:\", panel[\"ticker\"].nunique(), \"tickers;\", panel.shape, \"rows\")\n",
    "\n",
    "# === 2) Align to a common trading calendar (UBSG.SW as reference) ===\n",
    "REF = \"UBSG.SW\"\n",
    "ref_dates = (panel.loc[panel.ticker.eq(REF), [\"date\"]].drop_duplicates())\n",
    "panel = panel.merge(ref_dates, on=\"date\", how=\"inner\")\n",
    "print(\"After calendar alignment:\", panel.shape)\n",
    "\n",
    "# === 3) Basic returns + winsorization + rolling features + targets ===\n",
    "def add_features(g):\n",
    "    g = g.sort_values(\"date\").copy()\n",
    "\n",
    "    # Simple/log returns\n",
    "    g[\"ret_1d\"]    = g[\"adj_close\"].pct_change()\n",
    "    g[\"logret_1d\"] = np.log(g[\"adj_close\"]).diff()\n",
    "\n",
    "    # Winsorize (per-ticker tails)\n",
    "    lo, hi = g[\"ret_1d\"].quantile([0.005, 0.995]).values\n",
    "    g[\"ret_1d_w\"]    = g[\"ret_1d\"].clip(lo, hi)\n",
    "    g[\"logret_1d_w\"] = np.log1p(g[\"ret_1d_w\"])\n",
    "\n",
    "    # Rolling features on winsorized log returns + price features\n",
    "    for w in (5, 21, 63):\n",
    "        g[f\"rv_{w}d_w\"] = g[\"logret_1d_w\"].rolling(w).std() * np.sqrt(252)\n",
    "        g[f\"ma_{w}d\"]   = g[\"adj_close\"].rolling(w).mean()\n",
    "        g[f\"mom_{w}d\"]  = g[\"adj_close\"].pct_change(w)\n",
    "\n",
    "    # Forward-return targets\n",
    "    for h in (5, 21):\n",
    "        g[f\"y_fwdret_{h}d\"] = g[\"adj_close\"].pct_change(h).shift(-h)\n",
    "    return g\n",
    "\n",
    "panel_w = panel.groupby(\"ticker\", group_keys=False).apply(add_features)\n",
    "need = [\"ticker\",\"date\",\"adj_close\",\"ret_1d_w\",\"logret_1d_w\",\"rv_5d_w\",\"rv_21d_w\",\"rv_63d_w\",\"y_fwdret_5d\",\"y_fwdret_21d\"]\n",
    "panel_w = panel_w.dropna(subset=need).reset_index(drop=True)\n",
    "print(\"Enriched panel:\", panel_w.shape)\n",
    "\n",
    "# === 4) Save outputs (Parquet + CSV) ===\n",
    "# Note: requires 'pyarrow' or 'fastparquet' for Parquet I/O\n",
    "try:\n",
    "    panel_w.to_parquet(OUT_DIR / \"swiss_panel_ready.parquet\", index=False)\n",
    "except Exception as e:\n",
    "    print(\"Parquet save skipped (engine missing):\", e)\n",
    "panel_w.to_csv(OUT_DIR / \"swiss_panel_ready.csv\", index=False)\n",
    "\n",
    "close_wide = panel_w.pivot(index=\"date\", columns=\"ticker\", values=\"adj_close\").sort_index()\n",
    "try:\n",
    "    close_wide.to_parquet(OUT_DIR / \"swiss_close_wide_ready.parquet\")\n",
    "except Exception as e:\n",
    "    print(\"Parquet save skipped (engine missing):\", e)\n",
    "close_wide.to_csv(OUT_DIR / \"swiss_close_wide_ready.csv\")\n",
    "\n",
    "print(\"✅ Saved to:\", OUT_DIR)\n",
    "print(\"Wide shape:\", close_wide.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f575a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74439d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "financetech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
